{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgnUfKxWWjrt"
      },
      "source": [
        "# TRANSTAILOR METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5Fa39lWmsl"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0AOWS5EWoYC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EuydGGwWq5p"
      },
      "source": [
        "## Load model and dataset\n",
        "* Model: VGG16\n",
        "* Dataset: CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bh_Mq4xW3Z9",
        "outputId": "9944f9f1-111c-41c1-d67a-136730da8bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE: \" + str(device))\n",
        "\n",
        "# Load the VGG16 model\n",
        "model = torchvision.models.vgg16(pretrained=True)\n",
        "batch_size = 256\n",
        "\n",
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "kwargs = {'num_workers': 10, 'pin_memory': True} if device == 'cuda' else {}\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "\n",
        "# Freeze the parameters of the model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer of the model with a new layer that matches the number of classes in CIFAR10\n",
        "num_classes = 10\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWxtIR-tBLyK"
      },
      "source": [
        "## Finetune model based on target data CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAhreNjYXaHh",
        "outputId": "7f2ce609-36a0-43a6-c1a8-04a87bc8dd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Fine-tune the pre-trained model to generate W_s*===\n",
            "Epoch 0/10\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10;\n",
        "\n",
        "# Fine-tune the pre-trained model to generate W_s*\n",
        "print(\"\\n===Fine-tune the pre-trained model to generate W_s*===\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYGF5cN4W7o_"
      },
      "source": [
        "## Define and traing scaling factor `Î±`\n",
        "\n",
        "1.   Train the scaling factors using the target data (CIFAR10 in this case).\n",
        "2.   Transform the scaling factors to the filter importance using the Taylor expansion method.\n",
        "3.   Prune the filters based on the filter importance.\n",
        "4.   Fine-tune the pruned model using the target data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b0y-eNKdKHS"
      },
      "outputs": [],
      "source": [
        "def generate_scaling_factors(num_layers, num_filters):\n",
        "    scaling_factors = torch.zeros(num_layers, max(num_filters))\n",
        "    for i in range(num_layers):\n",
        "        scaling_factors[i, :num_filters[i]] = torch.rand(num_filters[i])\n",
        "    return scaling_factors\n",
        "\n",
        "num_layers = len(model.features)\n",
        "num_filters = [0] * num_layers\n",
        "\n",
        "for i in range(num_layers):\n",
        "    layer = model.features[i]\n",
        "    if isinstance(layer, torch.nn.Conv2d):\n",
        "        num_filters[i] = layer.out_channels\n",
        "\n",
        "alpha = generate_scaling_factors(num_layers, num_filters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i157onsiBK5g"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the output of each filter\n",
        "image, _ = next(iter(train_loader))\n",
        "\n",
        "filter_outputs = []\n",
        "\n",
        "for i in range(num_layers):\n",
        "    if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "        filter_outputs.append(torch.zeros(batch_size, num_filters[i], image.shape[2], image.shape[3]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcOq3txYfCCI"
      },
      "outputs": [],
      "source": [
        "for i in range(len(filter_outputs)):\n",
        "    print(filter_outputs[i].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "Rl0x_Gp8gauW",
        "outputId": "09f68cfa-57af-491c-8da8-31d5a92ffcfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Train the factors alpha by optimizing the loss function===\n",
            "Epoch 0/10\n",
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m         optimizer_alpha\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Transform alpha to the filter importance beta using the Taylor expansion method\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m beta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(outputs, alpha, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(outputs), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\colab-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    408\u001b[0m         grad_outputs_\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    412\u001b[0m         t_outputs,\n\u001b[0;32m    413\u001b[0m         grad_outputs_,\n\u001b[0;32m    414\u001b[0m         retain_graph,\n\u001b[0;32m    415\u001b[0m         create_graph,\n\u001b[0;32m    416\u001b[0m         inputs,\n\u001b[0;32m    417\u001b[0m         allow_unused,\n\u001b[0;32m    418\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    419\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    424\u001b[0m     ):\n",
            "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "num_epochs = 10;\n",
        "# filter_outputs = []\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the factors alpha by optimizing the loss function\n",
        "print(\"\\n===Train the factors alpha by optimizing the loss function===\")\n",
        "optimizer_alpha = torch.optim.SGD([alpha], lr=0.001, momentum=0.9)\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_size = inputs.shape[0]\n",
        "        optimizer_alpha.zero_grad()\n",
        "        outputs = inputs\n",
        "        for i in range(num_layers):\n",
        "            # print(\"LAYER \" + str(i) + \":\" + str((model.features[i])))\n",
        "            # print(\"ALPHA \" + str(alpha[i][:num_filters[i]].shape))\n",
        "\n",
        "            if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "                outputs = model.features[i](outputs)\n",
        "                # print(\"Outputs shape: \" + str(outputs.shape))\n",
        "\n",
        "                # Multiply the output of each filter by its corresponding scaling factor\n",
        "                alpha_i = alpha[i][:num_filters[i]]\n",
        "                for j in range(num_filters[i]):\n",
        "                    outputs[:, j, :, :] = outputs[:, j, :, :] * alpha_i[j]\n",
        "\n",
        "                # print(\"Output after multiplying alpha [\"+str(i)+\"]: \" + str(outputs.shape))\n",
        "            else:\n",
        "                outputs = model.features[i](outputs)\n",
        "        # outputs = model.classifier[6](model.features(inputs) * alpha)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "        classification_output = model.classifier(outputs)\n",
        "        loss = criterion(classification_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer_alpha.step()\n",
        "\n",
        "# Transform alpha to the filter importance beta using the Taylor expansion method\n",
        "beta = torch.abs(torch.autograd.grad(outputs, alpha, grad_outputs=torch.ones_like(outputs), create_graph=True)[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
