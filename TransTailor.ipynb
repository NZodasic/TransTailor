{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgnUfKxWWjrt"
      },
      "source": [
        "# TRANSTAILOR METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5Fa39lWmsl"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d0AOWS5EWoYC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EuydGGwWq5p"
      },
      "source": [
        "## Load model and dataset\n",
        "* Model: VGG16\n",
        "* Dataset: CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bh_Mq4xW3Z9",
        "outputId": "9944f9f1-111c-41c1-d67a-136730da8bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE: \" + str(device))\n",
        "\n",
        "# Load the VGG16 model\n",
        "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "batch_size = 64\n",
        "\n",
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "kwargs = {'num_workers': 10, 'pin_memory': True} if device == 'cuda' else {}\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# Replace the last layer of the model with a new layer that matches the number of classes in CIFAR10\n",
        "num_classes = 10\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 134301514\n",
            "trainable 134301514\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "print(\"total\",sum(p.numel() for p in model.parameters()))\n",
        "print(\"trainable\",sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWxtIR-tBLyK"
      },
      "source": [
        "## Finetune model based on target data CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the model from previous training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the model's parameters\n",
        "model.load_state_dict(torch.load('model_parameters_10_epochs.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we don't find any checkpoint, finetune for 10 epoches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAhreNjYXaHh",
        "outputId": "7f2ce609-36a0-43a6-c1a8-04a87bc8dd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Fine-tune the pre-trained model to generate W_s*===\n",
            "Epoch 0/10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Epoch 2/10\n",
            "Epoch 3/10\n",
            "Epoch 4/10\n",
            "Epoch 5/10\n",
            "Epoch 6/10\n",
            "Epoch 7/10\n",
            "Epoch 8/10\n",
            "Epoch 9/10\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10;\n",
        "\n",
        "# Fine-tune the pre-trained model to generate W_s*\n",
        "print(\"\\n===Fine-tune the pre-trained model to generate W_s*===\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save model for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model's parameters\n",
        "torch.save(model.state_dict(), 'model_parameters_10_epochs.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYGF5cN4W7o_"
      },
      "source": [
        "## Define and traing scaling factor `Î±`\n",
        "\n",
        "1.   Train the scaling factors using the target data (CIFAR10 in this case).\n",
        "2.   Transform the scaling factors to the filter importance using the Taylor expansion method.\n",
        "3.   Prune the filters based on the filter importance.\n",
        "4.   Fine-tune the pruned model using the target data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Init `scaling_factors`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load `scaling_factors` (if any)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('scaling_factor_10epoch.pkl', 'rb') as handle:\n",
        "    scaling_factors = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Init `scaling_factors`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 64\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 64\n",
            "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 128\n",
            "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 128\n",
            "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n"
          ]
        }
      ],
      "source": [
        "num_layers = len(model.features)\n",
        "scaling_factors = {}\n",
        "\n",
        "for i in range(num_layers):\n",
        "    layer = model.features[i]\n",
        "    if isinstance(layer, torch.nn.Conv2d):\n",
        "        print(layer,layer.out_channels)\n",
        "        # num_filters[i] = layer.out_channels\n",
        "        scaling_factors[i] = torch.rand((1,layer.out_channels,1,1), requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train `scaling_factors` by freezing filters' outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Train the factors alpha by optimizing the loss function===\n",
            "Epoch 0/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:52<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:53<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:56<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:56<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|ââââââââââ| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "# filter_outputs = []\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n===Train the factors alpha by optimizing the loss function===\")\n",
        "params_to_optimize = itertools.chain(scaling_factors[sf] for sf in scaling_factors.keys())\n",
        "optimizer_alpha = torch.optim.SGD(params_to_optimize, lr=0.001, momentum=0.9)\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    iter_count = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_size = inputs.shape[0]\n",
        "        optimizer_alpha.zero_grad()\n",
        "        outputs = inputs\n",
        "        outputs.requires_grad = False\n",
        "        for i in range(num_layers):\n",
        "            if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "                outputs = model.features[i](outputs)\n",
        "                outputs = outputs*scaling_factors[i].cuda()\n",
        "            else:\n",
        "                outputs = model.features[i](outputs)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "        classification_output = model.classifier(outputs)\n",
        "        loss = criterion(classification_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer_alpha.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Save `scaling_factors`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('scaling_factor_10epoch.pkl', 'wb') as handle:\n",
        "    pickle.dump(scaling_factors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transform `scaling_factors` to `importance_score`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Only run below cell if you load scaling factor from file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "importance_scores = {}\n",
        "num_layers = len(model.features)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = inputs\n",
        "    for i in range(num_layers):\n",
        "        if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "            outputs = model.features[i](outputs)\n",
        "            outputs = outputs*scaling_factors[i].cuda()\n",
        "        else:\n",
        "            outputs = model.features[i](outputs)\n",
        "\n",
        "    outputs = torch.flatten(outputs, 1)\n",
        "    classification_output = model.classifier(outputs)\n",
        "    loss = criterion(classification_output, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create `importance_scores` using Taylor expansion**\n",
        "\n",
        "$$\\beta_i^j=\\left|\\frac{\\partial\\mathcal{L}\\left(D_t;W_f^s\\bigodot\\alpha^*\\right)}{\\partial\\left(\\alpha^*\\right)_i^j}\\left(\\alpha^*\\right)_i^j\\right|$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, scaling_factor in scaling_factors.items():\n",
        "    first_order_derivative = torch.autograd.grad(loss, scaling_factor, retain_graph=True)[0]\n",
        "    importance_scores[i] = torch.abs(first_order_derivative * scaling_factor)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
