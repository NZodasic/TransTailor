{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFDvyf2oeodf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import time\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import vgg16\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.utils.prune as prune\n",
        "from heapq import nsmallest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "WYRHD6uteu6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Cub2011(Dataset):\n",
        "    base_folder = 'CUB_200_2011/images'\n",
        "    url = 'https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
        "    filename = 'CUB_200_2011.tgz'\n",
        "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.loader = default_loader\n",
        "        self.train = train\n",
        "\n",
        "        if download:\n",
        "            self._download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "    def _load_metadata(self):\n",
        "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
        "                             names=['img_id', 'filepath'])\n",
        "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
        "                                         sep=' ', names=['img_id', 'target'])\n",
        "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
        "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
        "\n",
        "        data = images.merge(image_class_labels, on='img_id')\n",
        "        self.data = data.merge(train_test_split, on='img_id')\n",
        "\n",
        "        if self.train:\n",
        "            self.data = self.data[self.data.is_training_img == 1]\n",
        "        else:\n",
        "            self.data = self.data[self.data.is_training_img == 0]\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        try:\n",
        "            self._load_metadata()\n",
        "        except Exception:\n",
        "            return False\n",
        "\n",
        "        for index, row in self.data.iterrows():\n",
        "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
        "            if not os.path.isfile(filepath):\n",
        "                print(filepath)\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def _download(self):\n",
        "        import tarfile\n",
        "\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "            return\n",
        "\n",
        "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
        "\n",
        "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
        "            tar.extractall(path=self.root)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx]\n",
        "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
        "        target = sample.target - 1\n",
        "        img = self.loader(path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ],
      "metadata": {
        "id": "bQRUN-3Me3rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CIFAR10(Dataset):\n",
        "#     base_folder = 'CIFAR_10/images'\n",
        "#     url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "#     # https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1'\n",
        "#     filename = 'cifar-10-python.tar.gz'\n",
        "#     tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "\n",
        "#     def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
        "#         self.root = os.path.expanduser(root)\n",
        "#         self.transform = transform\n",
        "#         self.loader = default_loader\n",
        "#         self.train = train\n",
        "\n",
        "#         if download:\n",
        "#             self._download()\n",
        "\n",
        "#         if not self._check_integrity():\n",
        "#             raise RuntimeError('Dataset not found or corrupted.' +\n",
        "#                                ' You can use download=True to download it')\n",
        "\n",
        "#     def _load_metadata(self):\n",
        "#         images = pd.read_csv(os.path.join(self.root, 'CIFAR_10', 'images.txt'), sep=' ',\n",
        "#                              names=['img_id', 'filepath'])\n",
        "#         image_class_labels = pd.read_csv(os.path.join(self.root, 'CIFAR_10', 'image_class_labels.txt'),\n",
        "#                                          sep=' ', names=['img_id', 'target'])\n",
        "#         train_test_split = pd.read_csv(os.path.join(self.root, 'CIFAR_10', 'train_test_split.txt'),\n",
        "#                                        sep=' ', names=['img_id', 'is_training_img'])\n",
        "\n",
        "#         data = images.merge(image_class_labels, on='img_id')\n",
        "#         self.data = data.merge(train_test_split, on='img_id')\n",
        "\n",
        "#         if self.train:\n",
        "#             self.data = self.data[self.data.is_training_img == 1]\n",
        "#         else:\n",
        "#             self.data = self.data[self.data.is_training_img == 0]\n",
        "\n",
        "#     def _check_integrity(self):\n",
        "#         try:\n",
        "#             self._load_metadata()\n",
        "#         except Exception:\n",
        "#             return False\n",
        "\n",
        "#         for index, row in self.data.iterrows():\n",
        "#             filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
        "#             if not os.path.isfile(filepath):\n",
        "#                 print(filepath)\n",
        "#                 return False\n",
        "#         return True\n",
        "\n",
        "#     def _download(self):\n",
        "#         import tarfile\n",
        "\n",
        "#         if self._check_integrity():\n",
        "#             print('Files already downloaded and verified')\n",
        "#             return\n",
        "\n",
        "#         download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
        "\n",
        "#         with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
        "#             tar.extractall(path=self.root)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sample = self.data.iloc[idx]\n",
        "#         path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
        "#         target = sample.target - 1\n",
        "#         img = self.loader(path)\n",
        "\n",
        "#         if self.transform is not None:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         return img, target"
      ],
      "metadata": {
        "id": "ZpdfUKcRh6G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomResizedCrop(224),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "0VaDKdc8e62_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Cub2011('.', train=True, transform=transform)\n",
        "val_ds = Cub2011('.s', train=False, transform=transform)\n",
        "\n",
        "ds = {'train': DataLoader(train_ds, batch_size=64, shuffle=True),\n",
        "      'val': DataLoader(val_ds, batch_size=64, shuffle=False)}\n",
        "\n",
        "ds_sizes = {'train': len(train_ds),\n",
        "            'val': len(val_ds)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5Y01emye9RR",
        "outputId": "23de4fc1-345b-4703-a60a-3be7118ab032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=40, nclas=200, patience=8):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    #best_acc = 0.0\n",
        "    best_bal_acc = 0.0\n",
        "\n",
        "    #early stopping\n",
        "    best_epoch = 0\n",
        "\n",
        "    epochs_bal_acc = []\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            CF = np.zeros((nclas,nclas)) # Confusion matrix\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in ds[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                for i in range(len(labels.data)):\n",
        "                    CF[labels.data[i]][preds[i]] +=1\n",
        "\n",
        "            #if phase == 'train':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / ds_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / ds_sizes[phase]\n",
        "            recalli = 0\n",
        "            for i in range(nclas):\n",
        "                TP = CF[i][i]\n",
        "                FN = 0\n",
        "                for j in range(nclas):\n",
        "                    if i!=j:\n",
        "                        FN+=CF[i][j]\n",
        "                if (TP+FN) !=0:\n",
        "                    recalli+= TP/(TP+FN)\n",
        "            epoch_bal_acc = recalli/nclas\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} Balanced Acc: {epoch_bal_acc:.4f}')\n",
        "            if phase == 'val':\n",
        "                epochs_bal_acc.append(epoch_bal_acc)\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_bal_acc > best_bal_acc:\n",
        "                best_bal_acc = epoch_bal_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                best_epoch = epoch\n",
        "\n",
        "            if phase == 'val' and epoch - best_epoch > patience:\n",
        "                print('Early stopping')\n",
        "                break\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Balanced Acc: {best_bal_acc:4f}')\n",
        "    print(epochs_bal_acc)\n",
        "\n",
        "    # load best model weights\n",
        "    #model.load_state_dict(best_model_wts)\n",
        "    return model, best_bal_acc"
      ],
      "metadata": {
        "id": "5EBxmoU3e_QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super(AlphaConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        # Difference: v2 uses torch.ones, v1 uses torch.rand\n",
        "        self.alpha = nn.Parameter(torch.ones(out_channels))  # v2 version\n",
        "        # self.alpha = nn.Parameter(torch.rand(out_channels))  # v1 version\n",
        "\n",
        "    def forward(self, x):\n",
        "        #return super(AlphaConv2d, self).forward(x) * self.alpha\n",
        "        return super(AlphaConv2d, self).forward(x) * self.alpha.unsqueeze(1).unsqueeze(2)"
      ],
      "metadata": {
        "id": "IGGs10tnfZsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_weights(m):\n",
        "    if type(m) == AlphaConv2d:\n",
        "        for module in m.named_parameters():\n",
        "            print(module)"
      ],
      "metadata": {
        "id": "G0P6t4uTfaPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg16(weights='IMAGENET1K_V1')\n",
        "model.classifier[6] = nn.Linear(4096, 200)"
      ],
      "metadata": {
        "id": "24VK5S8wffXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    if type(module) == nn.Conv2d:\n",
        "        new_module = AlphaConv2d(module.in_channels, module.out_channels, module.kernel_size, module.stride, module.padding, module.dilation, module.groups, True)\n",
        "        new_module.weight = module.weight\n",
        "        new_module.bias = module.bias\n",
        "        model.features[int(name.split('.')[1])] = new_module\n"
      ],
      "metadata": {
        "id": "8jt7kSypfgzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv = []  # Conv layers\n",
        "fc = []  # FC layers\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if type(module) == AlphaConv2d:\n",
        "        conv.append(module.alpha)\n",
        "    elif type(module) == nn.Linear:\n",
        "        fc.append(module.weight)\n",
        "        fc.append(module.bias)\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': conv},\n",
        "    {'params': fc, 'lr': 0.005}\n",
        "], weight_decay=0.005, momentum=0.9, lr=0.0005)\n",
        "\n",
        "adam_optimizer = torch.optim.Adam([\n",
        "    {'params': conv},\n",
        "    {'params': fc, 'lr': 0.01}\n",
        "], weight_decay=0.005, lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "LZKTG32Hfvjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_last = torch.optim.SGD(model.classifier[6].parameters(), lr=0.005, momentum=0.9, weight_decay=0.005)\n",
        "adam_optimizer_last = torch.optim.Adam(model.classifier[6].parameters(), lr=0.01, weight_decay=0.005)\n",
        "model = model.to(device)\n",
        "# Difference: v2 uses 20 epochs, v1 uses 60 epochs\n",
        "model, _ = train_model(model, criterion, optimizer_last, num_epochs=1, nclas=200)  # v2 version\n",
        "# model, _ = train_model(model, criterion, optimizer_last, num_epochs=60, nclas=200)  # v1 version\n"
      ],
      "metadata": {
        "id": "npQKD46Fpfjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "betterAcc = True\n",
        "previousAcc = 0.0\n",
        "\n",
        "# Difference: v2 locks non-AlphaConv2d modules' gradients\n",
        "for name, module in model.named_modules():\n",
        "    if type(module) != AlphaConv2d:\n",
        "        for param in module.parameters():\n",
        "            if param.requires_grad:\n",
        "                param.requires_grad = False\n",
        "\n",
        "while betterAcc:\n",
        "    for name, module in model.named_modules():\n",
        "        if type(module) == AlphaConv2d:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "            module.alpha.requires_grad = True\n",
        "\n",
        "    # Difference: v2 uses 20 epochs, v1 uses 60 epochs\n",
        "    model_ft, _ = train_model(model, criterion, optimizer, num_epochs=1, nclas=200)  # v2 version\n",
        "    # model_ft, _ = train_model(model, criterion, optimizer, num_epochs=60, nclas=200)  # v1 version\n",
        "\n",
        "    alpha_grad = {}\n",
        "    for inputs, labels in ds['train']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'alpha' in name:\n",
        "                if name.split('.')[0] + '.' + name.split('.')[1] not in alpha_grad:\n",
        "                    alpha_grad[name.split('.')[0] + '.' + name.split('.')[1]] = (param.grad / len(ds['train']))\n",
        "                else:\n",
        "                    alpha_grad[name.split('.')[0] + '.' + name.split('.')[1]] += (param.grad / len(ds['train']))\n",
        "\n",
        "    betas = []\n",
        "    for name, module in model_ft.named_modules():\n",
        "        if type(module) == AlphaConv2d:\n",
        "            module.alpha.data = torch.abs(alpha_grad[name] * module.alpha.data)  # Transform to beta\n",
        "            betas.extend(module.alpha)\n",
        "\n",
        "    PERC = 0.10\n",
        "    pruneVal = max(nsmallest(int(len(betas) * PERC), betas))\n",
        "\n",
        "    for name, module in model_ft.named_modules():\n",
        "        if type(module) == AlphaConv2d:\n",
        "            mask = module.alpha > pruneVal\n",
        "            print(f'Pruned {torch.sum((mask) == 0)} filters')\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2).unsqueeze(3).expand_as(module.weight.data)\n",
        "            prune.custom_from_mask(module, 'weight', mask)\n",
        "\n",
        "    for name, module in model_ft.named_modules():\n",
        "        if type(module) == AlphaConv2d:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = True\n",
        "            module.alpha.requires_grad = False\n",
        "\n",
        "    # Difference: v2 doesn't move model_ft to device (it's already there)\n",
        "    # model_ft = model_ft.to(device)  # v1 version\n",
        "\n",
        "    # Difference: v2 uses 40 epochs, v1 uses 60 epochs\n",
        "    model_ft, current_acc = train_model(model_ft, criterion, optimizer, num_epochs=40, nclas=200)  # v2 version\n",
        "    # model_ft, current_acc = train_model(model_ft, criterion, optimizer, num_epochs=60, nclas=200)  # v1 version\n",
        "\n",
        "    # Difference: v2 uses 0.3 as threshold, v1 uses 0.003\n",
        "    if current_acc - previousAcc > 0.3:  # v2 version\n",
        "    # if current_acc - previousAcc > 0.003:  # v1 version\n",
        "        previousAcc = current_acc\n",
        "        model = model_ft\n",
        "    else:\n",
        "        betterAcc = False\n",
        "    # Difference: v2 has a break statement here\n",
        "    break  # v2 version"
      ],
      "metadata": {
        "id": "V8iCCxD5klH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yc89mBt1mCLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}