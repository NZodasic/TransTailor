{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgnUfKxWWjrt"
      },
      "source": [
        "# TRANSTAILOR METHOD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5Fa39lWmsl"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d0AOWS5EWoYC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EuydGGwWq5p"
      },
      "source": [
        "## Load model and dataset\n",
        "* Model: VGG16\n",
        "* Dataset: CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Bh_Mq4xW3Z9",
        "outputId": "9944f9f1-111c-41c1-d67a-136730da8bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE: \" + str(device))\n",
        "\n",
        "# Load the VGG16 model\n",
        "model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "batch_size = 64\n",
        "\n",
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 train_dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "kwargs = {'num_workers': 12, 'pin_memory': True} if device == 'cuda' else {}\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "# Load test_dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "kwargs = {'num_workers': 12, 'pin_memory': True} if device == 'cuda' else {}\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "# Replace the last layer of the model with a new layer that matches the number of classes in CIFAR10\n",
        "num_classes = 10\n",
        "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 134301514\n",
            "trainable 134301514\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "print(\"total\",sum(p.numel() for p in model.parameters()))\n",
        "print(\"trainable\",sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWxtIR-tBLyK"
      },
      "source": [
        "## Target-aware pruning\n",
        "Finetune model based on target data CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the model from previous training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the model's parameters\n",
        "model.load_state_dict(torch.load('model_parameters_10_epochs.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If we don't find any checkpoint, finetune\n",
        "- In development phase, I finetune in 10 epochs\n",
        "- In the paper, authors finetune until 10% FLOPs of pre-trained model is pruned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAhreNjYXaHh",
        "outputId": "7f2ce609-36a0-43a6-c1a8-04a87bc8dd8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Fine-tune the pre-trained model to generate W_s*===\n",
            "Epoch 0/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:27<00:00,  3.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:30<00:00,  3.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:33<00:00,  3.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:31<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:31<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:33<00:00,  3.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:32<00:00,  3.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:32<00:00,  3.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:33<00:00,  3.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:32<00:00,  3.68it/s]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10;\n",
        "\n",
        "# Fine-tune the pre-trained model to generate W_s*\n",
        "print(\"\\n===Fine-tune the pre-trained model to generate W_s*===\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save model for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model's parameters\n",
        "torch.save(model.state_dict(), 'model_parameters_10_epochs.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create `scaling_factors` $\\alpha$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYGF5cN4W7o_"
      },
      "source": [
        "1.   Train the scaling factors $\\alpha$ using the target data (CIFAR10 in this case).\n",
        "2.   Transform the scaling factors $\\alpha$ to the filter importance score $\\beta$ using the Taylor expansion method.\n",
        "3.   Prune the filters based on the filter importance.\n",
        "4.   Fine-tune the pruned model using the target data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load `scaling_factors` $\\alpha$ (if any)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('scaling_factor_10epoch.pkl', 'rb') as handle:\n",
        "    scaling_factors = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train `scaling_factors` $\\alpha$\n",
        "Only run this if you do not load $\\alpha$ from file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Initialize `scaling_factors` $\\alpha$**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 64\n",
            "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 64\n",
            "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 128\n",
            "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 128\n",
            "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 256\n",
            "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n",
            "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) 512\n"
          ]
        }
      ],
      "source": [
        "num_layers = len(model.features)\n",
        "scaling_factors = {}\n",
        "\n",
        "for i in range(num_layers):\n",
        "    layer = model.features[i]\n",
        "    if isinstance(layer, torch.nn.Conv2d):\n",
        "        print(layer,layer.out_channels)\n",
        "        # num_filters[i] = layer.out_channels\n",
        "        scaling_factors[i] = torch.rand((1,layer.out_channels,1,1), requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train `scaling_factors` by freezing filters' outputs**\n",
        "\n",
        "In the paper, authors had trained for 60 epochs (on VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Train the factors alpha by optimizing the loss function===\n",
            "Epoch 0/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:52<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:53<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:56<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:56<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:55<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [03:54<00:00,  1.67it/s]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "# filter_outputs = []\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n===Train the factors alpha by optimizing the loss function===\")\n",
        "params_to_optimize = itertools.chain(scaling_factors[sf] for sf in scaling_factors.keys())\n",
        "optimizer_alpha = torch.optim.SGD(params_to_optimize, lr=0.001, momentum=0.9)\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    iter_count = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_size = inputs.shape[0]\n",
        "        optimizer_alpha.zero_grad()\n",
        "        outputs = inputs\n",
        "        outputs.requires_grad = False\n",
        "        for i in range(num_layers):\n",
        "            if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "                outputs = model.features[i](outputs)\n",
        "                outputs = outputs*scaling_factors[i].cuda()\n",
        "            else:\n",
        "                outputs = model.features[i](outputs)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "        classification_output = model.classifier(outputs)\n",
        "        loss = criterion(classification_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer_alpha.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Save `scaling_factors`** $\\alpha$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "with open('scaling_factor_10epoch.pkl', 'wb') as handle:\n",
        "    pickle.dump(scaling_factors, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transform `scaling_factors` $\\alpha$ to `importance_score` $\\beta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Only run below cell if you load scaling factor $\\alpha$ from file**. If you have manually trained $\\alpha$ above, **skip this step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "importance_scores = {}\n",
        "num_layers = len(model.features)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = inputs\n",
        "    for i in range(num_layers):\n",
        "        if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "            outputs = model.features[i](outputs)\n",
        "            outputs = outputs*scaling_factors[i].cuda()\n",
        "        else:\n",
        "            outputs = model.features[i](outputs)\n",
        "\n",
        "    outputs = torch.flatten(outputs, 1)\n",
        "    classification_output = model.classifier(outputs)\n",
        "    loss = criterion(classification_output, labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Create `importance_scores` $\\beta$ using Taylor expansion**\n",
        "\n",
        "$$\\beta_i^j=\\left|\\frac{\\partial\\mathcal{L}\\left(D_t;W_f^s\\bigodot\\alpha^*\\right)}{\\partial\\left(\\alpha^*\\right)_i^j}\\left(\\alpha^*\\right)_i^j\\right|$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Load `importance_scores` $\\beta$ from file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('importance_scores.pkl', 'rb') as handle:\n",
        "    importance_scores = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Transform `importance_scores` $\\beta$ from `scaling_factors` $\\alpha$**\n",
        "\n",
        "Do not run below cell if you have loaded $\\beta$ from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, scaling_factor in scaling_factors.items():\n",
        "    first_order_derivative = torch.autograd.grad(loss, scaling_factor, retain_graph=True)[0]\n",
        "    importance_scores[i] = torch.abs(first_order_derivative * scaling_factor).detach() #Freeze importance_scores[i] after calculating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Save `importance_scores` $\\beta$ from file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('importance_scores.pkl', 'wb') as handle:\n",
        "    pickle.dump(importance_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importance-aware Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prune the model by using importance_scores $\\beta$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define function to find the filter with lowest importance_score\n",
        "def find_filter_to_prune(importance_scores, pruned_filters):\n",
        "    min_value = float('inf')\n",
        "    min_filter = None\n",
        "    min_layer = None\n",
        "\n",
        "    for layer_index, scores_tensor in importance_scores.items():\n",
        "        for filter_index, score in enumerate(scores_tensor[0]):\n",
        "            # Check if the filter has already been pruned\n",
        "            if (layer_index, filter_index) in pruned_filters:\n",
        "                continue\n",
        "            \n",
        "            if score < min_value:\n",
        "                min_value = score.item()\n",
        "                min_filter = filter_index\n",
        "                min_layer = layer_index\n",
        "                if min_value == 0:\n",
        "                    break\n",
        "\n",
        "    return min_layer, min_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the original model: 134,301,514\n",
            "Next filter to prune - Layer: 17 Filter: 148\n"
          ]
        }
      ],
      "source": [
        "# Assuming model is the original VGG-16 model\n",
        "total_params_original = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters in the original model: {:,}'.format(total_params_original))\n",
        "\n",
        "# Initialize pruned_filters set if it's not already initialized\n",
        "if 'pruned_filters' not in locals():\n",
        "    pruned_filters = set()\n",
        "\n",
        "# Call the function to find the next filter to prune\n",
        "layer_to_prune, filter_to_prune = find_filter_to_prune(importance_scores, pruned_filters)\n",
        "\n",
        "# Now you have the layer and filter index to prune\n",
        "print(\"Next filter to prune - Layer:\", layer_to_prune, \"Filter:\", filter_to_prune)\n",
        "\n",
        "pruned_model = model\n",
        "\n",
        "pruned_layer = pruned_model.features[layer_to_prune]\n",
        "pruned_filter = pruned_layer.weight.data[filter_to_prune]\n",
        "\n",
        "with torch.no_grad():\n",
        "    pruned_layer.weight.data[filter_to_prune] = 0\n",
        "    pruned_layer.bias.data[filter_to_prune] = 0\n",
        "\n",
        "# After pruning, you can update the pruned_filters set\n",
        "pruned_filters.add((layer_to_prune, filter_to_prune))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of non-zero parameters after pruning: 134,292,294\n"
          ]
        }
      ],
      "source": [
        "def count_non_zero_parameters(model):\n",
        "    non_zero_params = sum(p.nonzero().size(0) for p in model.parameters() if p.requires_grad)\n",
        "    return non_zero_params\n",
        "\n",
        "non_zero_params_after_pruning = count_non_zero_parameters(pruned_model)\n",
        "print(\"Number of non-zero parameters after pruning: {:,}\".format(non_zero_params_after_pruning))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finetuning the model after pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===Train the factors alpha by optimizing the loss function===\n",
            "Epoch 0/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [03:42<00:00,  3.51it/s]\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1\n",
        "for param in pruned_model.parameters():\n",
        "    param.requires_grad = True\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\n===Train the factors alpha by optimizing the loss function===\")\n",
        "# params_to_optimize = itertools.chain(scaling_factors[sf] for sf in scaling_factors.keys())\n",
        "# optimizer_alpha = torch.optim.SGD(params_to_optimize, lr=0.001, momentum=0.9)\n",
        "\n",
        "optimizer = torch.optim.SGD(pruned_model.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch) + \"/\" + str(num_epochs))\n",
        "    iter_count = 0\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        batch_size = inputs.shape[0]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = inputs\n",
        "        # outputs.requires_grad = True\n",
        "        for i in range(num_layers):\n",
        "            if isinstance(model.features[i], torch.nn.Conv2d):\n",
        "                outputs = model.features[i](outputs)\n",
        "                outputs = outputs*importance_scores[i].cuda()\n",
        "            else:\n",
        "                outputs = model.features[i](outputs)\n",
        "        outputs = torch.flatten(outputs, 1)\n",
        "        classification_output = model.classifier(outputs)\n",
        "        loss = criterion(classification_output, labels)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculate the accuracy of the model after pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for class 0: 97.30%\n",
            "Accuracy for class 1: 98.00%\n",
            "Accuracy for class 2: 89.30%\n",
            "Accuracy for class 3: 72.90%\n",
            "Accuracy for class 4: 94.20%\n",
            "Accuracy for class 5: 88.60%\n",
            "Accuracy for class 6: 98.60%\n",
            "Accuracy for class 7: 97.00%\n",
            "Accuracy for class 8: 96.90%\n",
            "Accuracy for class 9: 92.30%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "for i in range(10):\n",
        "    accuracy = 100 * class_correct[i] / class_total[i]\n",
        "    print('Accuracy for class {}: {:.2f}%'.format(i, accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
